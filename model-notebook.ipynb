{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logistic_regression\n",
    "import pandas as pd\n",
    "\n",
    "# Loading data\n",
    "data = pd.read_csv(\"Data PCOS/data without infertility _final.csv\")\n",
    "\n",
    "# Create a data frame \n",
    "data = pd.DataFrame(data)\n",
    "del data[data.columns[-1]] # Removing last column (error)\n",
    "\n",
    "# Delete the paciente identification columns\n",
    "header_raw = list(data)\n",
    "header_del_elements_1 = ['Sl. No', 'Patient File No.']\n",
    "data_model_1 = data.copy()\n",
    "data_model_1.drop(columns=header_del_elements_1, inplace=True)\n",
    "\n",
    "# Convert to numpy array\n",
    "data_model_1 = data_model_1.to_numpy()\n",
    "y_1 = data_model_1[:, 0]\n",
    "X_1 = data_model_1[:, 1:]\n",
    "\n",
    "# Remove nan values from the data\n",
    "y_1 = y_1[np.squeeze(~np.isnan(X_1).any(axis=1))]\n",
    "X_1 = X_1[~np.isnan(X_1).any(axis=1)]\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_1, y_1, test_size=0.30, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-score normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_1 = np.mean(X_train_1, axis=0)\n",
    "train_std_1 = np.std(X_train_1, axis=0)\n",
    "X_train_norm_1 = (X_train_1 - train_mean_1) / train_std_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the gradient descent in the data using all the parameter in the linear form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.60   \n",
      "Iteration   45: Cost     0.22   \n",
      "Iteration   90: Cost     0.18   \n",
      "Iteration  135: Cost     0.17   \n",
      "Iteration  180: Cost     0.16   \n",
      "Iteration  225: Cost     0.16   \n",
      "Iteration  270: Cost     0.15   \n",
      "Iteration  315: Cost     0.15   \n",
      "Iteration  360: Cost     0.15   \n",
      "Iteration  405: Cost     0.15   \n",
      "Iteration  449: Cost     0.15   \n"
     ]
    }
   ],
   "source": [
    "w_1, b_1, J_history_1, w_history_1 = logistic_regression.gradient_descent_reg(X_train_norm_1, y_train_1, np.zeros((39,)), 0, 0.2, 450, 1e-5 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**\n",
    "\n",
    "Considering accuracy the percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before removing features: 84.0 %\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test data\n",
    "X_test_norm_1 = (X_test_1 - train_mean_1) / train_std_1\n",
    "\n",
    "# Compute the accurace\n",
    "f_wb_1, g_1 = logistic_regression.predict(X_test_norm_1, w_1, b_1)\n",
    "accuracy_1 = np.mean(np.where(f_wb_1 == y_test_1, 1, 0))\n",
    "print('Accuracy before removing features:',round(accuracy_1 ,3) * 100,'%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive predictive value(PPV)** is the probability that a person who test positive actually has the disease.\n",
    "The PPV is a good measure of confiability of a test, lets calculate it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictive value: 87.2 %\n"
     ]
    }
   ],
   "source": [
    "y_test_positive_1 = y_test_1[f_wb_1 == 1]\n",
    "ppv_1 = np.mean(y_test_positive_1)\n",
    "print('Positive predictive value:',round(ppv_1,3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative predictive value (NPV)** is the probability that a person who test negative actually not having the disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative predictive value: 82.6 %\n"
     ]
    }
   ],
   "source": [
    "y_test_negative_1 = y_test_1[f_wb_1 == 0]\n",
    "npv_1 = np.mean(1 - y_test_negative_1)\n",
    "print('Negative predictive value:',round(npv_1, 3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear model with all the parameter conclusion**\n",
    "\n",
    "As shown above, the model using all the parameters and just linear components have a low accurace of just 0.44, lets study the parameter to increase the prediction accurace.\n",
    "\n",
    "### Feature analysis for best fit\n",
    "Print features used in previous prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCOS (Y/N)', ' Age (yrs)', 'Weight (Kg)', 'Height(Cm) ', 'BMI', 'Blood Group', 'Pulse rate(bpm) ', 'RR (breaths/min)', 'Hb(g/dl)', 'Cycle(R/I)', 'Cycle length(days)', 'Marraige Status (Yrs)', 'Pregnant(Y/N)', 'No. of aborptions', 'FSH(mIU/mL)', 'LH(mIU/mL)', 'FSH/LH', 'Hip(inch)', 'Waist(inch)', 'Waist:Hip Ratio', 'TSH (mIU/L)', 'AMH(ng/mL)', 'PRL(ng/mL)', 'Vit D3 (ng/mL)', 'PRG(ng/mL)', 'RBS(mg/dl)', 'Weight gain(Y/N)', 'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)', 'Pimples(Y/N)', 'Fast food (Y/N)', 'Reg.Exercise(Y/N)', 'BP _Systolic (mmHg)', 'BP _Diastolic (mmHg)', 'Follicle No. (L)', 'Follicle No. (R)', 'Avg. F size (L) (mm)', 'Avg. F size (R) (mm)', 'Endometrium (mm)']\n"
     ]
    }
   ],
   "source": [
    "header = [element for element in header_raw if element not in header_del_elements_1]\n",
    "print(header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the data have clear no correlation with the study, so lets remove the following:\n",
    "* Age (1)\n",
    "* Pulse rate (7)\n",
    "* RR\n",
    "* Marraige Status\n",
    "\n",
    "Data remove because of correlation:\n",
    "* BMI (keep)\n",
    "  * Weight\n",
    "  * Height\n",
    "* Hip Ratio (keep)\n",
    "  * Hip\n",
    "  * Waist\n",
    "\n",
    "**Removing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.71   \n",
      "Iteration 30000: Cost     0.26   \n",
      "Iteration 60000: Cost     0.23   \n",
      "Iteration 90000: Cost     0.22   \n",
      "Iteration 120000: Cost     0.22   \n",
      "Iteration 150000: Cost     0.22   \n",
      "Iteration 180000: Cost     0.22   \n",
      "Iteration 210000: Cost     0.22   \n",
      "Iteration 240000: Cost     0.22   \n",
      "Iteration 270000: Cost     0.22   \n",
      "Iteration 299999: Cost     0.22   \n"
     ]
    }
   ],
   "source": [
    "# Creating parameter for a new model\n",
    "header_del_elements_2 = ['Sl. No', 'Patient File No.',' Age (yrs)', 'Pulse rate(bpm) ',\n",
    "'RR (breaths/min)', 'Marraige Status (Yrs)','Weight (Kg)', 'Height(Cm) ', 'Hip(inch)', \n",
    "'Waist(inch)', 'Fast food (Y/N)', 'Reg.Exercise(Y/N)','Blood Group']\n",
    "\n",
    "data_model_2 = data.copy()\n",
    "data_model_2.drop(columns=header_del_elements_2, inplace=True)\n",
    "\n",
    "# Convert to numpy array\n",
    "data_model_2 = data_model_2.to_numpy()\n",
    "\n",
    "y_2 = data_model_2[:, 0]\n",
    "X_2 = data_model_2[:, 1:]\n",
    "\n",
    "# Remove nan values from the data\n",
    "y_2 = y_2[np.squeeze(~np.isnan(X_2).any(axis=1))]\n",
    "X_2 = X_2[~np.isnan(X_2).any(axis=1)]\n",
    "\n",
    "# Split the data in train and test\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    X_2, y_2, test_size=0.30, random_state=0)\n",
    "\n",
    "\n",
    "# Normalize \n",
    "train_mean_2 = np.mean(X_train_2)\n",
    "train_std_2 = np.std(X_train_2)\n",
    "\n",
    "X_train_norm_2 = (X_train_2 - train_mean_2)/train_std_2\n",
    "\n",
    "# Apply gradient descent\n",
    "w_2, b_2, J_history_2, w_history_2  = logistic_regression.gradient_descent_reg(X_train_norm_2, y_train_2, \n",
    "np.zeros((X_train_norm_2.shape[1],)),0, 3, 300000, 0.7e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after removing features: 88.9 %\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test data\n",
    "X_test_norm_2 = (X_test_2 - train_mean_2) / train_std_2\n",
    "\n",
    "# Compute the accurace\n",
    "f_wb_2, g_2 = logistic_regression.predict(X_test_norm_2, w_2, b_2)\n",
    "accuracy_2 = np.mean(np.where(f_wb_2 == y_test_2, 1, 0))\n",
    "print('Accuracy after removing features:',round(accuracy_2, 3) * 100,'%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive predictive value(PPV)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictive value: 95.5 %\n"
     ]
    }
   ],
   "source": [
    "y_test_positive_2 = y_test_2[f_wb_2 == 1]\n",
    "ppv_2 = np.mean(y_test_positive_2)\n",
    "print('Positive predictive value:',round(ppv_2,3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative predictive value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative predictive value: 86.4 %\n"
     ]
    }
   ],
   "source": [
    "y_test_negative_2 = y_test_2[f_wb_2 == 0]\n",
    "npv_2 = np.mean(1 - y_test_negative_2)\n",
    "print('Negative predictive value:',round(npv_2, 3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "\n",
    "With help of the scikit learn library $mapping$ lets create polynomial features to fit data with nonlinear pattern.\n",
    "\n",
    "Obs: The feature selection from the second model will be used once it produce better predictions with less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.62   \n",
      "Iteration 7000: Cost     0.46   \n",
      "Iteration 14000: Cost     0.40   \n",
      "Iteration 21000: Cost     0.37   \n",
      "Iteration 28000: Cost     0.35   \n",
      "Iteration 35000: Cost     0.34   \n",
      "Iteration 42000: Cost     0.34   \n",
      "Iteration 49000: Cost     0.33   \n",
      "Iteration 56000: Cost     0.33   \n",
      "Iteration 63000: Cost     0.32   \n",
      "Iteration 69999: Cost     0.32   \n"
     ]
    }
   ],
   "source": [
    "# This code can take while to run \n",
    "\n",
    "# Split the data in train and test\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(\n",
    "    X_2, y_2, test_size=0.30, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_3 = poly.fit_transform(X_train_3)\n",
    "X_test_3 = poly.fit_transform(X_test_3)\n",
    "\n",
    "# Normalize\n",
    "train_mean_3 = np.mean(X_train_3)\n",
    "train_std_3 = np.std(X_train_3)\n",
    "\n",
    "X_train_norm_3 = (X_train_3 - train_mean_3) / train_std_3 \n",
    "\n",
    "# Apply gradient descent\n",
    "w_3, b_3, J_history_3, w_history_3  = logistic_regression.gradient_descent_reg(X_train_norm_3, y_train_3, \n",
    "np.zeros((X_train_norm_3.shape[1],)),0, 5, 70000, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after feature maping: 84.0 %\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test data\n",
    "X_test_norm_3 = (X_test_3 - train_mean_3) / train_std_3\n",
    "\n",
    "# Compute the accurace\n",
    "f_wb_3, g_3 = logistic_regression.predict(X_test_norm_3, w_3, b_3)\n",
    "accuracy_3 = np.mean(np.where(f_wb_3 == y_test_3, 1, 0))\n",
    "print('Accuracy after feature maping:',round(accuracy_3, 3) * 100,'%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive predictive value (PPV)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictive value: 97.1 %\n"
     ]
    }
   ],
   "source": [
    "y_test_positive_3 = y_test_3[f_wb_3 == 1.0]\n",
    "ppv_3 = np.mean(y_test_positive_3)\n",
    "print('Positive predictive value:',round(ppv_3,3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative predictive value (NVP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative predictive value: 80.5 %\n"
     ]
    }
   ],
   "source": [
    "y_test_negative_3 = y_test_3[f_wb_3 == 0]\n",
    "npv_3 = np.mean(1 - y_test_negative_3)\n",
    "print('Negative predictive value:',round(npv_3, 3) * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "First model\n",
    "* Accuracy = 84.0%\n",
    "* PVP = 87.2%\n",
    "* NVP = 82.6%\n",
    "\n",
    "Second model\n",
    "* Accuracy = 88.9%\n",
    "* PVP = 95.5%\n",
    "* NVP = 86.4%\n",
    "\n",
    "Third model\n",
    "* Accuracy = 84.0%\n",
    "* PVP = 97.1%\n",
    "* NVP = 80.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
